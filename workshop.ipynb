{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Society Term 2 Workshop 3 - Word Embedding\n",
    "Welcome to the third workshop in our deep learning series. In this workshop we begin to take a look at natural languague processing often denoted NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = \"theano\"\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "As the name implies natural language processing revolves around processing word data. This task comes with a number of difficulties. For instance, we unlike examples we looked at before position of words is often of great importance in languages so this has to be factored in somehow. We also need to consider the fact that context is also of importance and teaching a machine to recognise context within a text is no easy task. Today we will look at simple examples. \n",
    "\n",
    "The data we will consider is of text taken from movie reviews online. We shall try to train a model that picks up on whether we have a positive or negative sentiment about the movie the text refers to. \n",
    "\n",
    "We begin by defining a list of strings which will serve as our text data. This is defined below as ````text````. We also define a list called ````labels```` which indicate whether the sentiment is positive or negative. In this case ````1```` indicates a positive sentiment, whereas ````0```` indicates a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text\n",
    "text = [\"Enjoyable\",\n",
    "        \"Trash\",\n",
    "        \"Absolute banger!\",\n",
    "        \"Good\",\n",
    "        \"Thrilling\",\n",
    "        \"A complete bore\",\n",
    "        \"Sleep inducing\",\n",
    "        \"A must watch\",\n",
    "        \"Avoid at all costs\",\n",
    "        \"Exhausting\",\n",
    "        \"Never been better!\",\n",
    "        \"Too slow\",\n",
    "        \"Very interesting watch\",\n",
    "        \"Glowing performance\",\n",
    "        \"Frustrating\",\n",
    "        \"Dull\"]\n",
    "\n",
    "# Sentiment labels\n",
    "labels = [1,0,1,1,1,0,0,1,0,0,1,0,1,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Text\n",
    "Our text data needs to somehow be converted into a form that can be interpreted by a machine. Traditionally, most data we've dealt with in the past is some form of numerical data. It would then make sense if we looked for some way to encode this text somehow in a numerical form. Fortunately, for us Keras has the functionality we need to do this.\n",
    "\n",
    "We define a size for our vocabulary for the text in question. We choose this value so that it is an overestimate of the number of different words we have in our data. In this case ````50```` is chosen for ````vocab_size````. Next we define an empty list ````encoded_text```` to contain our encoded text. We will loop over all of the entries within the ````text```` list and pass them along with the ````vocab_size```` into the Keras ````one_hot```` function. This essentially just converts each word in a given string into an integer; one integer for each word. We have ````[]```` enclosing the function so that the output is a list. Essentially, the loop keeps adding the lists from the output of the ````one_hot```` function to the ````encoded_text```` list. What results is a list of lists of varying lengths. The length of each list simply corresponds to the number of words that were present in that particular entry. The results are then printed so we can see the what become of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated vocab size\n",
    "vocab_size = 50\n",
    "\n",
    "# Empty list for the encoded text\n",
    "encoded_text = []\n",
    "\n",
    "# Loop over every entry in list\n",
    "for i in range(len(text)):\n",
    "    # Converts phrase to set of integers\n",
    "    encoded_text += [one_hot(text[i], vocab_size)]\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to ensure all our data is of the same length. We define a ````max_length```` of ````4```` as this was the maximum number of words one entry contained. We call upon the ````pad_sequences```` function to take ````encoded_text```` and make sure each entry is of the length ````max_length````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length\n",
    "max_length = 4\n",
    "\n",
    "# Ensures sequences are all of the same length\n",
    "padded_text = pad_sequences(encoded_text, max_length)\n",
    "print(padded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Neural Network\n",
    "As is usually the case we begin defining the neural network architecture by specifiying that our model be of the ````Sequential```` type. The first layer we add will be an ````Embedding```` layer. These layers essentially create a projection of our word into a vector space. The first argument needs to be the vocabulary size which we defined as ````vocab_size````. The second argument specifies the number of dimensions the vector space in which we embed our words contains. The third argument; input length is chosen to be the ````max_length```` as this is the length of our ````padded_text````. \n",
    "\n",
    "Next, we ````Flatten```` the data as before. Here the only dense layer we are including is the output layer which contains ````1```` neuron and uses the ````\"sigmoid\"```` activation function. Note: the number of neurons in the output layer need only match the number of classes for the case of a ````softmax```` activation function. \n",
    "\n",
    "For our training parameters we use the ````\"adam\"```` optimizer as before. For loss we will use ````\"binary_crossentropy\"```` which is a suitable loss function in a binary classification problem where you've only two outcomes. As usual we will track ````\"accuracy\"```` for our ````metrics```` argument. As this is a small dataset we can run for ````epochs=200````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "\n",
    "# Platten data\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "# Training parameters\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model training\n",
    "model.fit(padded_text,labels,epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Here we create a test dataset to determine how well our model performs. As before we encode and pad our words in the same way as we did for the training set above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples\n",
    "test_text = [\"Fell asleep\",\n",
    "             \"Boring\",\n",
    "             \"A must see\",\n",
    "             \"Avoid this movie\",\n",
    "             \"Incredibly dull\",\n",
    "             \"Good performance\",\n",
    "             \"A bit slow\",\n",
    "             \"Really dull\"]\n",
    "# Test labels\n",
    "test_labels = [0,0,1,0,0,1,0,0]\n",
    "\n",
    "# List for encoded text data\n",
    "encoded_test = []\n",
    "\n",
    "# Loops over test data\n",
    "for i in range(len(test_text)):\n",
    "    # Converts phrase to set of integers\n",
    "    encoded_test += [one_hot(test_text[i],vocab_size)]\n",
    "\n",
    "# Ensures the sequences are of the same length\n",
    "padded_test = pad_sequences(encoded_test,max_length)\n",
    "print(padded_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test the accuracy of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates model performance\n",
    "loss, accuracy = model.evaluate(padded_test, test_labels)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
